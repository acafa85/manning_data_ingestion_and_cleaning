{"cells":[{"cell_type":"code","source":["display(\"Hello World\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48f39965-23d6-4dd6-83f0-065f3bf947ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"'Hello World'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["'Hello World'"]}}],"execution_count":0},{"cell_type":"code","source":["sc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49f0b50f-9748-4770-811c-4315296262be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=319565351198238#setting/sparkui/0703-130454-ufdaaswi/driver-5488963805572891328\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=319565351198238#setting/sparkui/0703-130454-ufdaaswi/driver-5488963805572891328\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["sc.getConf().getAll()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c76fe112-487b-4a17-a2b7-a4adb23aede1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: [('spark.databricks.preemption.enabled', 'true'),\n ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'),\n ('spark.driver.tempDirectory', '/local_disk0/tmp'),\n ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'),\n ('spark.databricks.managedCatalog.clientClassName',\n  'com.databricks.managedcatalog.ManagedCatalogClientImpl'),\n ('spark.hadoop.fs.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem'),\n ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.retry.limit', '20'),\n ('spark.sql.streaming.checkpointFileManagerClass',\n  'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'),\n ('spark.databricks.service.dbutils.repl.backend',\n  'com.databricks.dbconnect.ReplDBUtils'),\n ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'),\n ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'),\n ('spark.databricks.clusterSource', 'UI'),\n ('spark.hadoop.hive.server2.transport.mode', 'http'),\n ('spark.executor.memory', '8278m'),\n ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',\n  'true'),\n ('spark.databricks.clusterUsageTags.hailEnabled', 'false'),\n ('spark.hadoop.fs.mcfs-s3.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',\n  '11.0.x-scala2.12'),\n ('spark.databricks.clusterUsageTags.containerType', 'LXC'),\n ('spark.eventLog.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false'),\n ('spark.databricks.clusterUsageTags.driverInstanceId', 'i-0d781a239e3dae947'),\n ('spark.hadoop.hive.hmshandler.retry.interval', '2000'),\n ('spark.executor.tempDirectory', '/local_disk0/tmp'),\n ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'),\n ('spark.hadoop.fs.fcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.mapred.output.committer.class',\n  'com.databricks.backend.daemon.data.client.DirectOutputCommitter'),\n ('spark.hadoop.hive.server2.thrift.http.port', '10000'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.sql.allowMultipleContexts', 'false'),\n ('spark.home', '/databricks/spark'),\n ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'),\n ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'),\n ('spark.task.reaper.enabled', 'true'),\n ('spark.storage.memoryFraction', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '345161959944231'),\n ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '0'),\n ('spark.databricks.sql.configMapperClass',\n  'com.databricks.dbsql.config.SqlConfigMapperBridge'),\n ('spark.driver.maxResultSize', '4g'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'),\n ('spark.hadoop.fs.fcfs-s3.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.delta.multiClusterWrites.enabled', 'true'),\n ('spark.worker.cleanup.enabled', 'false'),\n ('spark.sql.legacy.createHiveTableByDefault', 'false'),\n ('spark.ui.port', '40001'),\n ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.attempts.maximum', '10'),\n ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',\n  'ebs_volume_type: GENERAL_PURPOSE_SSD\\n'),\n ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'),\n ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',\n  'false'),\n ('spark.hadoop.fs.fcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'),\n ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDestination', ''),\n ('spark.databricks.wsfsPublicPreview', 'true'),\n ('spark.cleaner.referenceTracking.blocking', 'false'),\n ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'false'),\n ('spark.databricks.clusterUsageTags.clusterState', 'Pending'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',\n  'false'),\n ('spark.databricks.tahoe.logStore.azure.class',\n  'com.databricks.tahoe.store.AzureLogStore'),\n ('spark.hadoop.fs.azure.skip.metrics', 'true'),\n ('spark.hadoop.fs.s3.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem'),\n ('spark.hadoop.hive.hmshandler.retry.attempts', '10'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.sql.sources.default', 'delta'),\n ('spark.databricks.clusterUsageTags.driverContainerPrivateIp',\n  '10.172.233.244'),\n ('spark.hadoop.fs.mcfs-gs.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterWorkers', '0'),\n ('spark.hadoop.fs.cpfs-s3n.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.cpfs-adl.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-abfss.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.passthrough.oauth.refresher.impl',\n  'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'),\n ('spark.sql.hive.metastore.sharedPrefixes',\n  'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'),\n ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'),\n ('spark.task.reaper.killTimeout', '60s'),\n ('spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider'),\n ('spark.hadoop.parquet.block.size.row.check.min', '10'),\n ('spark.hadoop.hive.server2.use.SSL', 'true'),\n ('spark.hadoop.fs.mcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND'),\n ('spark.app.startTime', '1656853567474'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'),\n ('spark.hadoop.hive.server2.keystore.path',\n  '/databricks/keys/jetty-ssl-driver-keystore.jks'),\n ('spark.hadoop.fs.elfs.impl.disable.cache', 'true'),\n ('spark.databricks.credential.redactor',\n  'com.databricks.logging.secrets.CredentialRedactorProxyImpl'),\n ('spark.databricks.clusterUsageTags.effectiveSparkVersion',\n  '11.0.x-scala2.12'),\n ('spark.databricks.clusterUsageTags.clusterPinned', 'false'),\n ('spark.databricks.acl.provider',\n  'com.databricks.sql.acl.ReflectionBackedAclProvider'),\n ('spark.extraListeners',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.sql.parquet.cacheMetadata', 'true'),\n ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'),\n ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'),\n ('spark.hadoop.fs.adl.impl', 'com.databricks.adl.AdlFileSystem'),\n ('spark.hadoop.fs.dbfs.impl',\n  'com.databricks.backend.daemon.data.client.DBFS'),\n ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'),\n ('spark.databricks.tahoe.logStore.class',\n  'com.databricks.tahoe.store.DelegatingLogStore'),\n ('libraryDownload.sleepIntervalSeconds', '5'),\n ('spark.databricks.cloudProvider', 'AWS'),\n ('spark.sql.hive.convertMetastoreParquet', 'true'),\n ('spark.executor.id', 'driver'),\n ('spark.databricks.service.dbutils.server.backend',\n  'com.databricks.dbconnect.SparkServerDBUtils'),\n ('spark.databricks.clusterUsageTags.workerEnvironmentId',\n  'default-worker-env'),\n ('spark.databricks.managedCatalog.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider'),\n ('spark.databricks.repl.enableClassFileCleanup', 'true'),\n ('spark.hadoop.fs.s3a.multipart.size', '10485760'),\n ('spark.databricks.clusterUsageTags.cloudProvider', 'AWS'),\n ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'),\n ('spark.r.sql.derby.temp.dir', '/tmp/Rtmp569P7r'),\n ('spark.executor.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'),\n ('spark.akka.frameSize', '256'),\n ('spark.hadoop.fs.s3a.fast.upload', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',\n  'true'),\n ('spark.sql.streaming.stopTimeout', '15s'),\n ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'),\n ('spark.hadoop.fs.wasbs.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'),\n ('spark.databricks.overrideDefaultCommitProtocol',\n  'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'),\n ('spark.driver.host', '10.172.233.244'),\n ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'),\n ('libraryDownload.timeoutSeconds', '180'),\n ('spark.hadoop.parquet.memory.pool.ratio', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'),\n ('spark.databricks.clusterUsageTags.userId', '345161959944231'),\n ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'),\n ('spark.hadoop.fs.s3a.block.size', '67108864'),\n ('spark.databricks.tahoe.logStore.gcp.class',\n  'com.databricks.tahoe.store.GCPLogStore'),\n ('spark.repl.class.uri', 'spark://10.172.233.244:40325/classes'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.sql.sources.commitProtocolClass',\n  'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'),\n ('spark.hadoop.fs.abfss.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem'),\n ('spark.hadoop.fs.fcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''),\n ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'),\n ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.hadoop.fs.file.impl',\n  'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'),\n ('spark.hadoop.fs.mcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '319565351198238'),\n ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-s3.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''),\n ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'),\n ('spark.rpc.message.maxSize', '256'),\n ('spark.hadoop.fs.elfs.impl',\n  'com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''),\n ('spark.hadoop.fs.fcfs-wasbs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.driverNfs.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',\n  'RDS_DIRECT'),\n ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'),\n ('spark.databricks.sparkContextId', '5488963805572891328'),\n ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.instanceProfileUsed', 'false'),\n ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'),\n ('spark.hadoop.fs.abfs.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem'),\n ('spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus',\n  'false'),\n ('spark.databricks.acl.scim.client',\n  'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'),\n ('spark.hadoop.databricks.s3.amazonS3Client.cache.enabled', 'false'),\n ('spark.hadoop.fs.adl.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.block.size.row.check.max', '10'),\n ('spark.hadoop.fs.s3a.connection.maximum', '200'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'),\n ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',\n  'com.amazonaws.auth.InstanceProfileCredentialsProvider'),\n ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'),\n ('spark.databricks.clusterUsageTags.driverInstancePrivateIp',\n  '10.172.237.252'),\n ('spark.repl.class.outputDir',\n  '/local_disk0/tmp/repl/spark-5488963805572891328-f900d899-5f2f-4d9c-bfb1-236d2ec8a77e'),\n ('spark.shuffle.reduceLocality.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.driverNodeType', 'dev-tier-node'),\n ('spark.hadoop.spark.sql.sources.outputCommitterClass',\n  'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'),\n ('spark.databricks.clusterUsageTags.driverPublicDns',\n  'ec2-54-245-73-239.us-west-2.compute.amazonaws.com'),\n ('spark.hadoop.fs.fcfs-abfs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.instanceBootstrapType', 'ssh'),\n ('spark.databricks.clusterUsageTags.driverContainerId',\n  '733c98dcdff64cb9a2742e3aa32df81c'),\n ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'),\n ('spark.databricks.driverNodeTypeId', 'dev-tier-node'),\n ('spark.sql.parquet.compression.codec', 'snappy'),\n ('spark.databricks.cloudfetch.hasRegionSupport', 'true'),\n ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',\n  'false'),\n ('spark.executor.extraClassPath',\n  '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----com_google_protobuf--descriptor_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----com_google_protobuf--timestamp_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----workspace_spark_3_3--common--kvstore--kvstore-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--network-common--network-common-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--network-shuffle--network-shuffle-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--sketch--sketch-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--tags--tags-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--unsafe--unsafe-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_3--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_3--core--proto-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--graphx--graphx-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--launcher--launcher-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sd\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.3_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.3_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.3_2.12_deploy.jar_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--auto-value-annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--checker-compat-qual_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--checker-qual_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--commons-codec_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--commons-logging_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--error_prone_annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--failureaccess_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--flogger-log4j2-backend_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--flogger-system-backend-shaded_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--flogger_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-shaded-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcsio_proto_library-speed-src_shaded_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-client-java6_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-client_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-services-iamcredentials_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-services-storage_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-extensions_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-http-client-apache-v2_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-http-client-gson_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-http-client_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-oauth-client_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-alts_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-api_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-auth_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-context_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-core_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-grpclb_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-protobuf_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-stub_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gson_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--guava_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--httpclient_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--httpcore_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--j2objc-annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--jackson-core_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--jsr305_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libgcs-connector-spark_3.3_2.12_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libgcsio-spark_3.3_2.12_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libgcsio_iam_proto_library-speed_shaded_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libgcsio_proto_library-speed_shaded_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libmeta-services.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libutil-hadoop-spark_3.3_2.12_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libutil-spark_3.3_2.12_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--listenablefuture_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--opencensus-api_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--perfmark-api_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--protobuf-java-util_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--protobuf-java_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-annotations_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-core_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-databind_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-api_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-impl_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-jackson_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--scalapb-090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/third_party--scalapb-090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/third_party--scalapb-090--protobuf--scalapb--scalapb_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/third_party--scalapb-090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/third_party--zeromq--jeromq_shaded.jar:/databricks/jars/third_party--zeromq--jnacl_shaded.jar:/databricks/jars/utils--process_utils-spark_3.3_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.3_2.12_deploy.jar'),\n ('spark.hadoop.fs.wasb.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.hadoop.fs.mcfs-abfss.impl.disable.cache', 'true'),\n ('spark.databricks.workerNodeTypeId', 'dev-tier-node'),\n ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeSize', '0'),\n ('spark.sparklyr-backend.threads', '1'),\n ('spark.hadoop.fs.fcfs-wasb.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.passthrough.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'),\n ('spark.databricks.session.share', 'false'),\n ('spark.databricks.clusterUsageTags.clusterResourceClass', 'default'),\n ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'),\n ('spark.driver.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'),\n ('spark.hadoop.fs.gs.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'CUSTOM'),\n ('spark.delta.sharing.profile.provider.class',\n  'io.delta.sharing.DeltaSharingCredentialsProvider'),\n ('spark.databricks.managedCatalog.gcs.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider'),\n ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeType',\n  'GENERAL_PURPOSE_SSD'),\n ('spark.hadoop.parquet.page.size.check.estimate', 'false'),\n ('spark.hadoop.spark.driverproxy.customHeadersToProperties',\n  'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name'),\n ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',\n  'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'),\n ('spark.driver.port', '40325'),\n ('spark.databricks.clusterUsageTags.attribute_tag_service', ''),\n ('spark.databricks.delta.preview.enabled', 'true'),\n ('spark.databricks.metrics.filesystem_io_metrics', 'true'),\n ('spark.databricks.cloudfetch.requesterClassName',\n  'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'),\n ('spark.master', 'local[8]'),\n ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'),\n ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'),\n ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeCount', '0'),\n ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'),\n ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'),\n ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'),\n ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'),\n ('spark.databricks.tahoe.logStore.aws.class',\n  'com.databricks.tahoe.store.S3LockBasedLogStore'),\n ('spark.speculation.quantile', '0.9'),\n ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'),\n ('spark.shuffle.manager', 'SORT'),\n ('spark.files.overwrite', 'true'),\n ('spark.databricks.credential.aws.secretKey.redactor',\n  'com.databricks.spark.util.AWSSecretKeyRedactorProxy'),\n ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',\n  'false'),\n ('spark.r.numRBackendThreads', '1'),\n ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.orgId', '319565351198238'),\n ('spark.sql.hive.metastore.version', '0.13.0'),\n ('spark.shuffle.service.port', '4048'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'),\n ('spark.databricks.acl.client',\n  'com.databricks.spark.sql.acl.client.SparkSqlAclClient'),\n ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'),\n ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'),\n ('spark.hadoop.fs.mcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.hadoop.fs.s3n.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem'),\n ('spark.databricks.clusterUsageTags.enableElasticDisk', 'false'),\n ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterNodeType', 'dev-tier-node'),\n ('spark.databricks.passthrough.adls.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'),\n ('spark.app.name', 'Databricks Shell'),\n ('spark.driver.allowMultipleContexts', 'false'),\n ('spark.hadoop.fs.AbstractFileSystem.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'),\n ('spark.rdd.compress', 'true'),\n ('spark.databricks.python.defaultPythonRepl', 'ipykernel'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''),\n ('spark.databricks.eventLog.dir', 'eventlogs'),\n ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'),\n ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'),\n ('spark.speculation', 'false'),\n ('spark.hadoop.databricks.dbfs.client.version', 'v1'),\n ('spark.hadoop.hive.server2.session.check.interval', '60000'),\n ('spark.sql.hive.convertCTAS', 'true'),\n ('spark.hadoop.fs.s3a.max.total.tasks', '1000'),\n ('spark.hadoop.spark.sql.parquet.output.committer.class',\n  'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'),\n ('spark.hadoop.fs.s3a.fast.upload.default', 'true'),\n ('spark.databricks.clusterUsageTags.clusterGeneration', '0'),\n ('spark.hadoop.fs.mlflowdbfs.impl',\n  'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'),\n ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'),\n ('spark.speculation.multiplier', '3'),\n ('spark.app.id', 'local-1656853571525'),\n ('spark.storage.blockManagerTimeoutIntervalMs', '300000'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',\n  'default-worker-env'),\n ('spark.sparkr.use.daemon', 'false'),\n ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'),\n ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'),\n ('spark.databricks.clusterUsageTags.sparkVersion', '11.0.x-scala2.12'),\n ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'),\n ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'),\n ('spark.hadoop.fs.s3a.threads.max', '136'),\n ('spark.r.backendConnectionTimeout', '604800'),\n ('spark.databricks.clusterUsageTags.clusterId', '0703-130454-ufdaaswi'),\n ('spark.hadoop.hive.server2.idle.session.timeout', '900000'),\n ('spark.databricks.redactor',\n  'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'),\n ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '120'),\n ('spark.hadoop.fs.s3a.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem'),\n ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'us-west-2'),\n ('spark.logConf', 'true'),\n ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'),\n ('spark.hadoop.hive.server2.enable.doAs', 'false'),\n ('eventLog.rolloverIntervalSeconds', '3600'),\n ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'),\n ('spark.shuffle.memoryFraction', '0.2'),\n ('spark.databricks.clusterUsageTags.clusterAllTags',\n  '[{\"key\":\"Name\",\"value\":\"ce-worker\"}]'),\n ('spark.databricks.clusterUsageTags.clusterName', 'manning_project'),\n ('spark.hadoop.fs.dbfsartifacts.impl',\n  'com.databricks.backend.daemon.data.client.DBFSV1'),\n ('spark.hadoop.fs.cpfs-s3a.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.containerZoneId', 'us-west-2c'),\n ('spark.hadoop.fs.s3a.connection.timeout', '50000'),\n ('spark.databricks.clusterUsageTags.region', 'us-west-2'),\n ('spark.databricks.clusterUsageTags.clusterSpotBidPricePercent', '100'),\n ('spark.files.useFetchCache', 'false')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: [('spark.databricks.preemption.enabled', 'true'),\n ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'),\n ('spark.driver.tempDirectory', '/local_disk0/tmp'),\n ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'),\n ('spark.databricks.managedCatalog.clientClassName',\n  'com.databricks.managedcatalog.ManagedCatalogClientImpl'),\n ('spark.hadoop.fs.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem'),\n ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.retry.limit', '20'),\n ('spark.sql.streaming.checkpointFileManagerClass',\n  'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'),\n ('spark.databricks.service.dbutils.repl.backend',\n  'com.databricks.dbconnect.ReplDBUtils'),\n ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'),\n ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'),\n ('spark.databricks.clusterSource', 'UI'),\n ('spark.hadoop.hive.server2.transport.mode', 'http'),\n ('spark.executor.memory', '8278m'),\n ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',\n  'true'),\n ('spark.databricks.clusterUsageTags.hailEnabled', 'false'),\n ('spark.hadoop.fs.mcfs-s3.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',\n  '11.0.x-scala2.12'),\n ('spark.databricks.clusterUsageTags.containerType', 'LXC'),\n ('spark.eventLog.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false'),\n ('spark.databricks.clusterUsageTags.driverInstanceId', 'i-0d781a239e3dae947'),\n ('spark.hadoop.hive.hmshandler.retry.interval', '2000'),\n ('spark.executor.tempDirectory', '/local_disk0/tmp'),\n ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'),\n ('spark.hadoop.fs.fcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.mapred.output.committer.class',\n  'com.databricks.backend.daemon.data.client.DirectOutputCommitter'),\n ('spark.hadoop.hive.server2.thrift.http.port', '10000'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.sql.allowMultipleContexts', 'false'),\n ('spark.home', '/databricks/spark'),\n ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'),\n ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'),\n ('spark.task.reaper.enabled', 'true'),\n ('spark.storage.memoryFraction', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '345161959944231'),\n ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '0'),\n ('spark.databricks.sql.configMapperClass',\n  'com.databricks.dbsql.config.SqlConfigMapperBridge'),\n ('spark.driver.maxResultSize', '4g'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'),\n ('spark.hadoop.fs.fcfs-s3.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.delta.multiClusterWrites.enabled', 'true'),\n ('spark.worker.cleanup.enabled', 'false'),\n ('spark.sql.legacy.createHiveTableByDefault', 'false'),\n ('spark.ui.port', '40001'),\n ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.attempts.maximum', '10'),\n ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',\n  'ebs_volume_type: GENERAL_PURPOSE_SSD\\n'),\n ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'),\n ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',\n  'false'),\n ('spark.hadoop.fs.fcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'),\n ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDestination', ''),\n ('spark.databricks.wsfsPublicPreview', 'true'),\n ('spark.cleaner.referenceTracking.blocking', 'false'),\n ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'false'),\n ('spark.databricks.clusterUsageTags.clusterState', 'Pending'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',\n  'false'),\n ('spark.databricks.tahoe.logStore.azure.class',\n  'com.databricks.tahoe.store.AzureLogStore'),\n ('spark.hadoop.fs.azure.skip.metrics', 'true'),\n ('spark.hadoop.fs.s3.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem'),\n ('spark.hadoop.hive.hmshandler.retry.attempts', '10'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.sql.sources.default', 'delta'),\n ('spark.databricks.clusterUsageTags.driverContainerPrivateIp',\n  '10.172.233.244'),\n ('spark.hadoop.fs.mcfs-gs.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterWorkers', '0'),\n ('spark.hadoop.fs.cpfs-s3n.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.cpfs-adl.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-abfss.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.passthrough.oauth.refresher.impl',\n  'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'),\n ('spark.sql.hive.metastore.sharedPrefixes',\n  'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'),\n ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'),\n ('spark.task.reaper.killTimeout', '60s'),\n ('spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider'),\n ('spark.hadoop.parquet.block.size.row.check.min', '10'),\n ('spark.hadoop.hive.server2.use.SSL', 'true'),\n ('spark.hadoop.fs.mcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND'),\n ('spark.app.startTime', '1656853567474'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'),\n ('spark.hadoop.hive.server2.keystore.path',\n  '/databricks/keys/jetty-ssl-driver-keystore.jks'),\n ('spark.hadoop.fs.elfs.impl.disable.cache', 'true'),\n ('spark.databricks.credential.redactor',\n  'com.databricks.logging.secrets.CredentialRedactorProxyImpl'),\n ('spark.databricks.clusterUsageTags.effectiveSparkVersion',\n  '11.0.x-scala2.12'),\n ('spark.databricks.clusterUsageTags.clusterPinned', 'false'),\n ('spark.databricks.acl.provider',\n  'com.databricks.sql.acl.ReflectionBackedAclProvider'),\n ('spark.extraListeners',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.sql.parquet.cacheMetadata', 'true'),\n ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'),\n ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'),\n ('spark.hadoop.fs.adl.impl', 'com.databricks.adl.AdlFileSystem'),\n ('spark.hadoop.fs.dbfs.impl',\n  'com.databricks.backend.daemon.data.client.DBFS'),\n ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'),\n ('spark.databricks.tahoe.logStore.class',\n  'com.databricks.tahoe.store.DelegatingLogStore'),\n ('libraryDownload.sleepIntervalSeconds', '5'),\n ('spark.databricks.cloudProvider', 'AWS'),\n ('spark.sql.hive.convertMetastoreParquet', 'true'),\n ('spark.executor.id', 'driver'),\n ('spark.databricks.service.dbutils.server.backend',\n  'com.databricks.dbconnect.SparkServerDBUtils'),\n ('spark.databricks.clusterUsageTags.workerEnvironmentId',\n  'default-worker-env'),\n ('spark.databricks.managedCatalog.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider'),\n ('spark.databricks.repl.enableClassFileCleanup', 'true'),\n ('spark.hadoop.fs.s3a.multipart.size', '10485760'),\n ('spark.databricks.clusterUsageTags.cloudProvider', 'AWS'),\n ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'),\n ('spark.r.sql.derby.temp.dir', '/tmp/Rtmp569P7r'),\n ('spark.executor.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'),\n ('spark.akka.frameSize', '256'),\n ('spark.hadoop.fs.s3a.fast.upload', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',\n  'true'),\n ('spark.sql.streaming.stopTimeout', '15s'),\n ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'),\n ('spark.hadoop.fs.wasbs.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'),\n ('spark.databricks.overrideDefaultCommitProtocol',\n  'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'),\n ('spark.driver.host', '10.172.233.244'),\n ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'),\n ('libraryDownload.timeoutSeconds', '180'),\n ('spark.hadoop.parquet.memory.pool.ratio', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'),\n ('spark.databricks.clusterUsageTags.userId', '345161959944231'),\n ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'),\n ('spark.hadoop.fs.s3a.block.size', '67108864'),\n ('spark.databricks.tahoe.logStore.gcp.class',\n  'com.databricks.tahoe.store.GCPLogStore'),\n ('spark.repl.class.uri', 'spark://10.172.233.244:40325/classes'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.sql.sources.commitProtocolClass',\n  'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'),\n ('spark.hadoop.fs.abfss.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem'),\n ('spark.hadoop.fs.fcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''),\n ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'),\n ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.hadoop.fs.file.impl',\n  'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'),\n ('spark.hadoop.fs.mcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '319565351198238'),\n ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-s3.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''),\n ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'),\n ('spark.rpc.message.maxSize', '256'),\n ('spark.hadoop.fs.elfs.impl',\n  'com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''),\n ('spark.hadoop.fs.fcfs-wasbs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.driverNfs.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',\n  'RDS_DIRECT'),\n ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'),\n ('spark.databricks.sparkContextId', '5488963805572891328'),\n ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.instanceProfileUsed', 'false'),\n ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'),\n ('spark.hadoop.fs.abfs.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem'),\n ('spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus',\n  'false'),\n ('spark.databricks.acl.scim.client',\n  'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'),\n ('spark.hadoop.databricks.s3.amazonS3Client.cache.enabled', 'false'),\n ('spark.hadoop.fs.adl.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.block.size.row.check.max', '10'),\n ('spark.hadoop.fs.s3a.connection.maximum', '200'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'),\n ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',\n  'com.amazonaws.auth.InstanceProfileCredentialsProvider'),\n ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'),\n ('spark.databricks.clusterUsageTags.driverInstancePrivateIp',\n  '10.172.237.252'),\n ('spark.repl.class.outputDir',\n  '/local_disk0/tmp/repl/spark-5488963805572891328-f900d899-5f2f-4d9c-bfb1-236d2ec8a77e'),\n ('spark.shuffle.reduceLocality.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.driverNodeType', 'dev-tier-node'),\n ('spark.hadoop.spark.sql.sources.outputCommitterClass',\n  'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'),\n ('spark.databricks.clusterUsageTags.driverPublicDns',\n  'ec2-54-245-73-239.us-west-2.compute.amazonaws.com'),\n ('spark.hadoop.fs.fcfs-abfs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.instanceBootstrapType', 'ssh'),\n ('spark.databricks.clusterUsageTags.driverContainerId',\n  '733c98dcdff64cb9a2742e3aa32df81c'),\n ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'),\n ('spark.databricks.driverNodeTypeId', 'dev-tier-node'),\n ('spark.sql.parquet.compression.codec', 'snappy'),\n ('spark.databricks.cloudfetch.hasRegionSupport', 'true'),\n ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',\n  'false'),\n ('spark.executor.extraClassPath',\n  '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----com_google_protobuf--descriptor_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----com_google_protobuf--timestamp_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.3-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----workspace_spark_3_3--common--kvstore--kvstore-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--network-common--network-common-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--network-shuffle--network-shuffle-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--sketch--sketch-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--tags--tags-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--common--unsafe--unsafe-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_3--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_3--core--proto-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--graphx--graphx-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--launcher--launcher-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.12.189.jar:/databricks/jars/----workspace_spark_3_3--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sd\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.3_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.3_2.12_deploy_shaded.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.3.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.3.jar:/databricks/jars/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.3_2.12_deploy.jar_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--animal-sniffer-annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--auto-value-annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--checker-compat-qual_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--checker-qual_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--commons-codec_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--commons-logging_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--conscrypt-openjdk-uber_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--error_prone_annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--failureaccess_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--flogger-log4j2-backend_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--flogger-system-backend-shaded_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--flogger_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-shaded-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcsio_proto_library-speed-src_shaded_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-client-jackson2_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-client-java6_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-client_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-services-iamcredentials_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-api-services-storage_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-auth-library-credentials_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-auth-library-oauth2-http_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-extensions_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-http-client-apache-v2_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-http-client-gson_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-http-client-jackson2_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-http-client_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-oauth-client-java6_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--google-oauth-client_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-alts_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-api_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-auth_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-context_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-core_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-grpclb_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-netty-shaded_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-protobuf-lite_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-protobuf_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--grpc-stub_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gson_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--guava_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--httpclient_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--httpcore_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--j2objc-annotations_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--jackson-core_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--jsr305_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libgcs-connector-spark_3.3_2.12_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libgcsio-spark_3.3_2.12_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libgcsio_iam_proto_library-speed_shaded_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libgcsio_proto_library-speed_shaded_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libmeta-services.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libutil-hadoop-spark_3.3_2.12_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--libutil-spark_3.3_2.12_spark_3.3_2.12_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--listenablefuture_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--opencensus-api_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--opencensus-contrib-http-util_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--perfmark-api_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--proto-google-common-protos_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--proto-google-iam-v1_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--protobuf-java-util_shaded.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--protobuf-java_shaded.jar:/databricks/jars/third_party--jackson--guava_only_shaded.jar:/databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:/databricks/jars/third_party--jackson--jsr305_only_shaded.jar:/databricks/jars/third_party--jackson--paranamer_only_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-client_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-http_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-io_shaded.jar:/databricks/jars/third_party--jetty-client--jetty-util_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-annotations_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-core_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jackson-databind_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-api_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-impl_shaded.jar:/databricks/jars/third_party--jsonwebtoken--jjwt-jackson_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:/databricks/jars/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:/databricks/jars/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:/databricks/jars/third_party--scalapb-090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/third_party--scalapb-090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/third_party--scalapb-090--protobuf--scalapb--scalapb_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/third_party--scalapb-090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/third_party--zeromq--jeromq_shaded.jar:/databricks/jars/third_party--zeromq--jnacl_shaded.jar:/databricks/jars/utils--process_utils-spark_3.3_2.12_deploy.jar:/databricks/jars/workflow--workflow-spark_3.3_2.12_deploy.jar'),\n ('spark.hadoop.fs.wasb.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.hadoop.fs.mcfs-abfss.impl.disable.cache', 'true'),\n ('spark.databricks.workerNodeTypeId', 'dev-tier-node'),\n ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeSize', '0'),\n ('spark.sparklyr-backend.threads', '1'),\n ('spark.hadoop.fs.fcfs-wasb.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.passthrough.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'),\n ('spark.databricks.session.share', 'false'),\n ('spark.databricks.clusterUsageTags.clusterResourceClass', 'default'),\n ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'),\n ('spark.driver.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'),\n ('spark.hadoop.fs.gs.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'CUSTOM'),\n ('spark.delta.sharing.profile.provider.class',\n  'io.delta.sharing.DeltaSharingCredentialsProvider'),\n ('spark.databricks.managedCatalog.gcs.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider'),\n ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeType',\n  'GENERAL_PURPOSE_SSD'),\n ('spark.hadoop.parquet.page.size.check.estimate', 'false'),\n ('spark.hadoop.spark.driverproxy.customHeadersToProperties',\n  'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name'),\n ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',\n  'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'),\n ('spark.driver.port', '40325'),\n ('spark.databricks.clusterUsageTags.attribute_tag_service', ''),\n ('spark.databricks.delta.preview.enabled', 'true'),\n ('spark.databricks.metrics.filesystem_io_metrics', 'true'),\n ('spark.databricks.cloudfetch.requesterClassName',\n  'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'),\n ('spark.master', 'local[8]'),\n ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'),\n ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'),\n ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeCount', '0'),\n ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'),\n ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'),\n ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'),\n ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'),\n ('spark.databricks.tahoe.logStore.aws.class',\n  'com.databricks.tahoe.store.S3LockBasedLogStore'),\n ('spark.speculation.quantile', '0.9'),\n ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'),\n ('spark.shuffle.manager', 'SORT'),\n ('spark.files.overwrite', 'true'),\n ('spark.databricks.credential.aws.secretKey.redactor',\n  'com.databricks.spark.util.AWSSecretKeyRedactorProxy'),\n ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',\n  'false'),\n ('spark.r.numRBackendThreads', '1'),\n ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.orgId', '319565351198238'),\n ('spark.sql.hive.metastore.version', '0.13.0'),\n ('spark.shuffle.service.port', '4048'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'),\n ('spark.databricks.acl.client',\n  'com.databricks.spark.sql.acl.client.SparkSqlAclClient'),\n ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'),\n ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'),\n ('spark.hadoop.fs.mcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.hadoop.fs.s3n.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem'),\n ('spark.databricks.clusterUsageTags.enableElasticDisk', 'false'),\n ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterNodeType', 'dev-tier-node'),\n ('spark.databricks.passthrough.adls.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'),\n ('spark.app.name', 'Databricks Shell'),\n ('spark.driver.allowMultipleContexts', 'false'),\n ('spark.hadoop.fs.AbstractFileSystem.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'),\n ('spark.rdd.compress', 'true'),\n ('spark.databricks.python.defaultPythonRepl', 'ipykernel'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''),\n ('spark.databricks.eventLog.dir', 'eventlogs'),\n ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'),\n ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'),\n ('spark.speculation', 'false'),\n ('spark.hadoop.databricks.dbfs.client.version', 'v1'),\n ('spark.hadoop.hive.server2.session.check.interval', '60000'),\n ('spark.sql.hive.convertCTAS', 'true'),\n ('spark.hadoop.fs.s3a.max.total.tasks', '1000'),\n ('spark.hadoop.spark.sql.parquet.output.committer.class',\n  'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'),\n ('spark.hadoop.fs.s3a.fast.upload.default', 'true'),\n ('spark.databricks.clusterUsageTags.clusterGeneration', '0'),\n ('spark.hadoop.fs.mlflowdbfs.impl',\n  'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'),\n ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'),\n ('spark.speculation.multiplier', '3'),\n ('spark.app.id', 'local-1656853571525'),\n ('spark.storage.blockManagerTimeoutIntervalMs', '300000'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',\n  'default-worker-env'),\n ('spark.sparkr.use.daemon', 'false'),\n ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'),\n ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'),\n ('spark.databricks.clusterUsageTags.sparkVersion', '11.0.x-scala2.12'),\n ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'),\n ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'),\n ('spark.hadoop.fs.s3a.threads.max', '136'),\n ('spark.r.backendConnectionTimeout', '604800'),\n ('spark.databricks.clusterUsageTags.clusterId', '0703-130454-ufdaaswi'),\n ('spark.hadoop.hive.server2.idle.session.timeout', '900000'),\n ('spark.databricks.redactor',\n  'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'),\n ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '120'),\n ('spark.hadoop.fs.s3a.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem'),\n ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'us-west-2'),\n ('spark.logConf', 'true'),\n ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'),\n ('spark.hadoop.hive.server2.enable.doAs', 'false'),\n ('eventLog.rolloverIntervalSeconds', '3600'),\n ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'),\n ('spark.shuffle.memoryFraction', '0.2'),\n ('spark.databricks.clusterUsageTags.clusterAllTags',\n  '[{\"key\":\"Name\",\"value\":\"ce-worker\"}]'),\n ('spark.databricks.clusterUsageTags.clusterName', 'manning_project'),\n ('spark.hadoop.fs.dbfsartifacts.impl',\n  'com.databricks.backend.daemon.data.client.DBFSV1'),\n ('spark.hadoop.fs.cpfs-s3a.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.containerZoneId', 'us-west-2c'),\n ('spark.hadoop.fs.s3a.connection.timeout', '50000'),\n ('spark.databricks.clusterUsageTags.region', 'us-west-2'),\n ('spark.databricks.clusterUsageTags.clusterSpotBidPricePercent', '100'),\n ('spark.files.useFetchCache', 'false')]"]}}],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"326a0d50-bf9f-4321-ba9f-a7a5afe5f5b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=319565351198238#setting/sparkui/0703-130454-ufdaaswi/driver-5488963805572891328\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=319565351198238#setting/sparkui/0703-130454-ufdaaswi/driver-5488963805572891328\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls(\"/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f05e570-098b-48a0-a9c4-7126813d6a6b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: [FileInfo(path='dbfs:/datalake/curated/', name='curated/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/datalake/raw/', name='raw/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/datalake/serving/', name='serving/', size=0, modificationTime=0)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: [FileInfo(path='dbfs:/datalake/curated/', name='curated/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/datalake/raw/', name='raw/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/datalake/serving/', name='serving/', size=0, modificationTime=0)]"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/datalake\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7226570c-5f5b-40db-a2a7-b7fd19c46818"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[15]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[15]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/datalake/raw\")\ndbutils.fs.mkdirs(\"/datalake/curated\")\ndbutils.fs.mkdirs(\"/datalake/serving\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b63b7c48-7915-4794-b30e-17d843e2bb77"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[20]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[20]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls(\"/datalake\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1429364-1ed6-4a76-98de-320a405a6aea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[22]: [FileInfo(path='dbfs:/datalake/curated/', name='curated/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/datalake/raw/', name='raw/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/datalake/serving/', name='serving/', size=0, modificationTime=0)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[22]: [FileInfo(path='dbfs:/datalake/curated/', name='curated/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/datalake/raw/', name='raw/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/datalake/serving/', name='serving/', size=0, modificationTime=0)]"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"153e27cc-82fd-4972-a244-947caab3db8e"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"notebook_1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1429676018600784}},"nbformat":4,"nbformat_minor":0}
